{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 1986\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 1920 ACTION 1 NEXT REWARD 0 NEXT STATE 1921\n",
            "CURR STATE 1921 ACTION -1 NEXT REWARD 0 NEXT STATE 1928\n",
            "CURR STATE 1928 ACTION -1 NEXT REWARD 0 NEXT STATE 1935\n",
            "CURR STATE 1935 ACTION -1 NEXT REWARD 0 NEXT STATE 1926\n",
            "CURR STATE 1926 ACTION 0 NEXT REWARD 0 NEXT STATE 1926\n",
            "CURR STATE 1926 ACTION -1 NEXT REWARD 0 NEXT STATE 1925\n",
            "CURR STATE 1925 ACTION -1 NEXT REWARD 0 NEXT STATE 1796\n",
            "CURR STATE 1796 ACTION 0 NEXT REWARD 0 NEXT STATE 1668\n",
            "CURR STATE 1668 ACTION 0 NEXT REWARD 0 NEXT STATE 1540\n",
            "CURR STATE 1540 ACTION -1 NEXT REWARD 0 NEXT STATE 1411\n",
            "CURR STATE 1411 ACTION 0 NEXT REWARD 0 NEXT STATE 1291\n",
            "CURR STATE 1291 ACTION 0 NEXT REWARD 0 NEXT STATE 1171\n",
            "CURR STATE 1171 ACTION 1 NEXT REWARD 0 NEXT STATE 1052\n",
            "CURR STATE 1052 ACTION 0 NEXT REWARD 0 NEXT STATE 924\n",
            "CURR STATE 924 ACTION -1 NEXT REWARD 0 NEXT STATE 795\n",
            "CURR STATE 795 ACTION 0 NEXT REWARD 0 NEXT STATE 675\n",
            "CURR STATE 675 ACTION 0 NEXT REWARD 0 NEXT STATE 555\n",
            "CURR STATE 555 ACTION 1 NEXT REWARD 0 NEXT STATE 436\n",
            "CURR STATE 436 ACTION 1 NEXT REWARD 0 NEXT STATE 309\n",
            "CURR STATE 309 ACTION 1 NEXT REWARD 0 NEXT STATE 174\n",
            "CURR STATE 174 ACTION -1 NEXT REWARD 0 NEXT STATE 165\n",
            "CURR STATE 165 ACTION 1 NEXT REWARD 0 NEXT STATE 30\n",
            "CURR STATE 30 ACTION 0 NEXT REWARD 0 NEXT STATE 22\n",
            "CURR STATE 22 ACTION -1 NEXT REWARD 0 NEXT STATE 13\n",
            "CURR STATE 13 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION -1 NEXT REWARD 0 NEXT STATE 3\n",
            "CURR STATE 3 ACTION -1 NEXT REWARD 0 NEXT STATE 10\n",
            "CURR STATE 10 ACTION -1 NEXT REWARD 0 NEXT STATE 17\n",
            "CURR STATE 17 ACTION 0 NEXT REWARD 0 NEXT STATE 153\n",
            "CURR STATE 153 ACTION 0 NEXT REWARD 0 NEXT STATE 289\n",
            "CURR STATE 289 ACTION -1 NEXT REWARD 0 NEXT STATE 424\n",
            "CURR STATE 424 ACTION 0 NEXT REWARD 0 NEXT STATE 552\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import random\n",
        "import csv\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)  \n",
        "\n",
        "log_file = \"log-random.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\"])\n",
        "\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        writer.writerow([curr_state, curr_action, next_reward, next_state])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 3 ACTION 1 NEXT REWARD 0 NEXT STATE 12\n",
            "CURR STATE 12 ACTION 1 NEXT REWARD 0 NEXT STATE 13\n",
            "CURR STATE 13 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION 1 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 0 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 0 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 0 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION 0 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION 1 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 1 NEXT REWARD 0 NEXT STATE 6\n",
            "CURR STATE 6 ACTION 1 NEXT REWARD 0 NEXT STATE 7\n",
            "CURR STATE 7 ACTION 0 NEXT REWARD 0 NEXT STATE 135\n",
            "CURR STATE 135 ACTION -1 NEXT REWARD 0 NEXT STATE 262\n",
            "CURR STATE 262 ACTION 1 NEXT REWARD 0 NEXT STATE 263\n",
            "CURR STATE 263 ACTION -1 NEXT REWARD 0 NEXT STATE 390\n",
            "CURR STATE 390 ACTION 1 NEXT REWARD 0 NEXT STATE 391\n",
            "CURR STATE 391 ACTION -1 NEXT REWARD 0 NEXT STATE 518\n",
            "CURR STATE 518 ACTION 1 NEXT REWARD 0 NEXT STATE 519\n",
            "CURR STATE 519 ACTION 1 NEXT REWARD 0 NEXT STATE 640\n",
            "CURR STATE 640 ACTION 1 NEXT REWARD 0 NEXT STATE 769\n",
            "CURR STATE 769 ACTION 0 NEXT REWARD 0 NEXT STATE 905\n",
            "CURR STATE 905 ACTION 1 NEXT REWARD 0 NEXT STATE 1042\n",
            "CURR STATE 1042 ACTION 0 NEXT REWARD 0 NEXT STATE 1050\n",
            "CURR STATE 1050 ACTION 1 NEXT REWARD 0 NEXT STATE 1059\n",
            "CURR STATE 1059 ACTION 0 NEXT REWARD 0 NEXT STATE 939\n",
            "CURR STATE 939 ACTION -1 NEXT REWARD 0 NEXT STATE 818\n",
            "CURR STATE 818 ACTION -1 NEXT REWARD 0 NEXT STATE 825\n",
            "CURR STATE 825 ACTION -1 NEXT REWARD 0 NEXT STATE 960\n",
            "CURR STATE 960 ACTION 0 NEXT REWARD 1 NEXT STATE 1088\n",
            "CURR STATE 1088 ACTION 0 NEXT REWARD 0 NEXT STATE 1088\n",
            "CURR STATE 1088 ACTION -1 NEXT REWARD 0 NEXT STATE 1088\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import csv\n",
        "import random\n",
        "\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0   \n",
        "gamma = 0.9   \n",
        "\n",
        "def get_Q(state, action):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    return Q[state][action]\n",
        "\n",
        "def set_Q(state, action, value):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    Q[state][action] = value\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)\n",
        "\n",
        "log_filename = \"q-random.tsv\"\n",
        "with open(log_filename, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "            old_value = get_Q(curr_state, curr_action)\n",
        "            \n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_Q = max([get_Q(next_state, a) for a in next_actions])\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_Q - old_value)\n",
        "            set_Q(curr_state, curr_action, new_value)\n",
        "            \n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0   \n",
        "gamma = 0.9   \n",
        "epsilon = 0.25\n",
        "\n",
        "def get_Q(state, action):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    return Q[state][action]\n",
        "\n",
        "def set_Q(state, action, value):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    Q[state][action] = value\n",
        "\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        q_values = [get_Q(state, a) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "log_filename = \"q-greedy.tsv\"\n",
        "with open(log_filename, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "            \n",
        "            old_value = get_Q(curr_state, curr_action)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_Q = max([get_Q(next_state, a) for a in next_actions])\n",
        "            \n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_Q - old_value)\n",
        "            set_Q(curr_state, curr_action, new_value)\n",
        "            \n",
        "            \n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "states = sorted(Q.keys())\n",
        "\n",
        "with open(\"policy-greedy.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\",\"action\"])\n",
        "    for state in states:\n",
        "        actions = simulator.get_actions(state)\n",
        "        q_values = [Q[state][a] for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        best_action = random.choice(best_actions)\n",
        "        writer.writerow([state, best_action])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.05  \n",
        "\n",
        "def get_Q(state, action):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    return Q[state][action]\n",
        "\n",
        "def set_Q(state, action, value):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0.0 for a in simulator.get_actions(state)}\n",
        "    Q[state][action] = value\n",
        "\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        q_values = [get_Q(state, a) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "num_episodes = 500\n",
        "max_steps = 1024\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    for init_state in simulator.initial_states:\n",
        "        curr_state = init_state\n",
        "        for step in range(max_steps):\n",
        "            actions = simulator.get_actions(curr_state)\n",
        "            curr_action = epsilon_greedy_policy(curr_state, actions)\n",
        "            next_reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "            \n",
        "            old_value = get_Q(curr_state, curr_action)\n",
        "            max_next_Q = max([get_Q(next_state, a) for a in simulator.get_actions(next_state)])\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_Q - old_value)\n",
        "            set_Q(curr_state, curr_action, new_value)\n",
        "            \n",
        "            curr_state = next_state\n",
        "            if next_reward > 0:\n",
        "                break  \n",
        "\n",
        "all_states = range(simulator.resolution * simulator.resolution * 8)\n",
        "with open(\"policy-optimal.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\",\"action\"])\n",
        "    for state in all_states:\n",
        "        actions = simulator.get_actions(state)\n",
        "        q_values = [get_Q(state, a) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        best_action = random.choice(best_actions)\n",
        "        writer.writerow([state, best_action])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "acknowledgements = [\n",
        "['1. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html', \n",
        " 'To learn more about Q learning fundamentals'],\n",
        "['2. https://davidstarsilver.wordpress.com/teaching/',\n",
        " 'Week 2 slides/code explain deterministic Q-learning, α=1, γ<1 updates'],\n",
        "['3. http://incompleteideas.net/book/RLbook2020.pdf', 'Chapter 6- Q learning'],\n",
        "['4. https://gymnasium.farama.org/tutorials/training_agents/frozenlake_q_learning/#sphx-glr-tutorials-training-agents-frozenlake-q-learning-py', 'Learning more about Q Learning Code']\n",
        "]\n",
        "\n",
        "\n",
        "columns = ['Source', 'Used For/Reason']\n",
        "\n",
        "ack_df = pd.DataFrame(acknowledgements, columns=columns)\n",
        "\n",
        "ack_df\n",
        "\n",
        "\n",
        "ack_df.to_csv(\"acknowledgements.txt\", sep=\"\\t\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
