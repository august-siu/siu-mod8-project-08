{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 2042\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 2044 ACTION -1 NEXT REWARD 0 NEXT STATE 1915\n",
            "CURR STATE 1915 ACTION -1 NEXT REWARD 0 NEXT STATE 1786\n",
            "CURR STATE 1786 ACTION 1 NEXT REWARD 0 NEXT STATE 1787\n",
            "CURR STATE 1787 ACTION -1 NEXT REWARD 0 NEXT STATE 1658\n",
            "CURR STATE 1658 ACTION -1 NEXT REWARD 0 NEXT STATE 1657\n",
            "CURR STATE 1657 ACTION 1 NEXT REWARD 0 NEXT STATE 1786\n",
            "CURR STATE 1786 ACTION 0 NEXT REWARD 0 NEXT STATE 1786\n",
            "CURR STATE 1786 ACTION 1 NEXT REWARD 0 NEXT STATE 1787\n",
            "CURR STATE 1787 ACTION 1 NEXT REWARD 0 NEXT STATE 1660\n",
            "CURR STATE 1660 ACTION -1 NEXT REWARD 0 NEXT STATE 1531\n",
            "CURR STATE 1531 ACTION 1 NEXT REWARD 0 NEXT STATE 1404\n",
            "CURR STATE 1404 ACTION 1 NEXT REWARD 0 NEXT STATE 1277\n",
            "CURR STATE 1277 ACTION 1 NEXT REWARD 0 NEXT STATE 1142\n",
            "CURR STATE 1142 ACTION 1 NEXT REWARD 0 NEXT STATE 1135\n",
            "CURR STATE 1135 ACTION 0 NEXT REWARD 0 NEXT STATE 1255\n",
            "CURR STATE 1255 ACTION 1 NEXT REWARD 0 NEXT STATE 1368\n",
            "CURR STATE 1368 ACTION -1 NEXT REWARD 0 NEXT STATE 1503\n",
            "CURR STATE 1503 ACTION -1 NEXT REWARD 0 NEXT STATE 1622\n",
            "CURR STATE 1622 ACTION 1 NEXT REWARD 0 NEXT STATE 1615\n",
            "CURR STATE 1615 ACTION 1 NEXT REWARD 0 NEXT STATE 1728\n",
            "CURR STATE 1728 ACTION 1 NEXT REWARD 0 NEXT STATE 1857\n",
            "CURR STATE 1857 ACTION 0 NEXT REWARD 0 NEXT STATE 1993\n",
            "CURR STATE 1993 ACTION 1 NEXT REWARD 0 NEXT STATE 2002\n",
            "CURR STATE 2002 ACTION 1 NEXT REWARD 0 NEXT STATE 2011\n",
            "CURR STATE 2011 ACTION 1 NEXT REWARD 0 NEXT STATE 1892\n",
            "CURR STATE 1892 ACTION 0 NEXT REWARD 0 NEXT STATE 1764\n",
            "CURR STATE 1764 ACTION 1 NEXT REWARD 0 NEXT STATE 1637\n",
            "CURR STATE 1637 ACTION -1 NEXT REWARD 0 NEXT STATE 1500\n",
            "CURR STATE 1500 ACTION 1 NEXT REWARD 0 NEXT STATE 1373\n",
            "CURR STATE 1373 ACTION -1 NEXT REWARD 0 NEXT STATE 1236\n",
            "CURR STATE 1236 ACTION 0 NEXT REWARD 0 NEXT STATE 1108\n",
            "CURR STATE 1108 ACTION 0 NEXT REWARD 0 NEXT STATE 980\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import random\n",
        "import csv\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)  \n",
        "\n",
        "log_file = \"log-random.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\"])\n",
        "\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        writer.writerow([curr_state, curr_action, next_reward, next_state])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 1149 ACTION -1 NEXT REWARD 0 NEXT STATE 1012\n",
            "CURR STATE 1012 ACTION 0 NEXT REWARD 0 NEXT STATE 884\n",
            "CURR STATE 884 ACTION 0 NEXT REWARD 0 NEXT STATE 756\n",
            "CURR STATE 756 ACTION 1 NEXT REWARD 0 NEXT STATE 629\n",
            "CURR STATE 629 ACTION 0 NEXT REWARD 0 NEXT STATE 493\n",
            "CURR STATE 493 ACTION -1 NEXT REWARD 0 NEXT STATE 356\n",
            "CURR STATE 356 ACTION -1 NEXT REWARD 0 NEXT STATE 227\n",
            "CURR STATE 227 ACTION 0 NEXT REWARD 0 NEXT STATE 107\n",
            "CURR STATE 107 ACTION 0 NEXT REWARD 0 NEXT STATE 115\n",
            "CURR STATE 115 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION -1 NEXT REWARD 0 NEXT STATE 121\n",
            "CURR STATE 121 ACTION 0 NEXT REWARD 0 NEXT STATE 249\n",
            "CURR STATE 249 ACTION -1 NEXT REWARD 0 NEXT STATE 376\n",
            "CURR STATE 376 ACTION -1 NEXT REWARD 0 NEXT STATE 511\n",
            "CURR STATE 511 ACTION -1 NEXT REWARD 0 NEXT STATE 630\n",
            "CURR STATE 630 ACTION -1 NEXT REWARD 0 NEXT STATE 621\n",
            "CURR STATE 621 ACTION -1 NEXT REWARD 0 NEXT STATE 484\n",
            "CURR STATE 484 ACTION -1 NEXT REWARD 0 NEXT STATE 355\n",
            "CURR STATE 355 ACTION -1 NEXT REWARD 0 NEXT STATE 234\n",
            "CURR STATE 234 ACTION 0 NEXT REWARD 0 NEXT STATE 242\n",
            "CURR STATE 242 ACTION 1 NEXT REWARD 0 NEXT STATE 251\n",
            "CURR STATE 251 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION -1 NEXT REWARD 0 NEXT STATE 121\n",
            "CURR STATE 121 ACTION 0 NEXT REWARD 0 NEXT STATE 249\n",
            "CURR STATE 249 ACTION -1 NEXT REWARD 0 NEXT STATE 376\n",
            "CURR STATE 376 ACTION 0 NEXT REWARD 0 NEXT STATE 504\n",
            "CURR STATE 504 ACTION 1 NEXT REWARD 0 NEXT STATE 633\n",
            "CURR STATE 633 ACTION -1 NEXT REWARD 0 NEXT STATE 760\n",
            "CURR STATE 760 ACTION -1 NEXT REWARD 0 NEXT STATE 895\n",
            "CURR STATE 895 ACTION -1 NEXT REWARD 0 NEXT STATE 1014\n",
            "CURR STATE 1014 ACTION 0 NEXT REWARD 0 NEXT STATE 1006\n",
            "CURR STATE 1006 ACTION 0 NEXT REWARD 0 NEXT STATE 998\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import csv\n",
        "\n",
        "Q = {}  \n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "log_file = \"q-random.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "            \n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "        #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "\n",
        "        if next_reward > 0:\n",
        "            break\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 32768 rows.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "num_states = 128\n",
        "num_actions = 4\n",
        "steps_per_state = 256  \n",
        "epsilon = 0.25         \n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "def get_next_reward_state(curr_state, action):\n",
        "    \"\"\"\n",
        "    Example deterministic environment:\n",
        "    next_state wraps around and reward=1 for final state\n",
        "    \"\"\"\n",
        "    next_state = (curr_state + (action+1) * 3) % num_states\n",
        "    reward = 1 if next_state == num_states - 1 else 0\n",
        "    return reward, next_state\n",
        "\n",
        "def epsilon_greedy_policy(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(num_actions) \n",
        "    else:\n",
        "        return np.argmax(Q[state])             \n",
        "\n",
        "rows = []\n",
        "\n",
        "for initial_state in range(num_states):\n",
        "    curr_state = initial_state\n",
        "    for step in range(steps_per_state):\n",
        "        action = epsilon_greedy_policy(curr_state)\n",
        "        \n",
        "        reward, next_state = get_next_reward_state(curr_state, action)\n",
        "        \n",
        "        old_value = Q[curr_state][action]\n",
        "        new_value = old_value + alpha * (reward + gamma * np.max(Q[next_state]) - old_value)\n",
        "        \n",
        "        rows.append([curr_state, action, next_state, reward, old_value, new_value])\n",
        "        \n",
        "        Q[curr_state][action] = new_value\n",
        "        \n",
        "        curr_state = next_state\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['curr_state', 'action', 'next_state', 'reward', 'old_value', 'new_value'])\n",
        "df.to_csv('q-greedy.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(f\"Generated {len(rows)} rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy-greedy.tsv generated for 128 states\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "num_states = 128\n",
        "num_actions = 4\n",
        "\n",
        "policy_rows = []\n",
        "\n",
        "for state in range(num_states):\n",
        "    best_action = int(np.argmax(Q[state])) \n",
        "    policy_rows.append([state, best_action])\n",
        "\n",
        "policy_df = pd.DataFrame(policy_rows, columns=['state', 'action'])\n",
        "policy_df.to_csv('policy-greedy.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(f\"policy-greedy.tsv generated for {len(policy_rows)} states\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy-optimal.tsv generated for 128 states\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "num_states = 128\n",
        "num_actions = 4\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1  \n",
        "max_steps = 256\n",
        "num_episodes = 512\n",
        "\n",
        "Q_opt = np.zeros((num_states, num_actions))\n",
        "\n",
        "def get_next_reward_state(curr_state, action):\n",
        "    next_state = (curr_state + (action+1) * 3) % num_states\n",
        "    reward = 1 if next_state == num_states - 1 else 0\n",
        "    return reward, next_state\n",
        "\n",
        "def epsilon_greedy_policy_opt(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(num_actions)\n",
        "    else:\n",
        "        return int(np.argmax(Q_opt[state]))\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    for initial_state in range(num_states):\n",
        "        curr_state = initial_state\n",
        "        for step in range(max_steps):\n",
        "            action = epsilon_greedy_policy_opt(curr_state)\n",
        "            reward, next_state = get_next_reward_state(curr_state, action)\n",
        "\n",
        "            old_value = Q_opt[curr_state][action]\n",
        "            new_value = old_value + alpha * (reward + gamma * np.max(Q_opt[next_state]) - old_value)\n",
        "            Q_opt[curr_state][action] = new_value\n",
        "\n",
        "            curr_state = next_state\n",
        "            if reward > 0:\n",
        "                break \n",
        "\n",
        "policy_opt_rows = []\n",
        "for state in range(num_states):\n",
        "    best_action = int(np.argmax(Q_opt[state]))\n",
        "    policy_opt_rows.append([state, best_action])\n",
        "\n",
        "policy_opt_df = pd.DataFrame(policy_opt_rows, columns=['state', 'action'])\n",
        "policy_opt_df.to_csv('policy-optimal.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(f\"policy-optimal.tsv generated for {len(policy_opt_rows)} states\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "acknowledgements = [\n",
        "['1. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html', \n",
        " 'To learn more about Q learning fundamentals'],\n",
        "['2. https://davidstarsilver.wordpress.com/teaching/',\n",
        " 'Week 2 slides/code explain deterministic Q-learning, α=1, γ<1 updates'],\n",
        "['3. http://incompleteideas.net/book/RLbook2020.pdf', 'Chapter 6- Q learning'],\n",
        "['4. https://gymnasium.farama.org/tutorials/training_agents/frozenlake_q_learning/#sphx-glr-tutorials-training-agents-frozenlake-q-learning-py', 'Learning more about Q Learning Code']\n",
        "]\n",
        "\n",
        "\n",
        "columns = ['Source', 'Used For/Reason']\n",
        "\n",
        "ack_df = pd.DataFrame(acknowledgements, columns=columns)\n",
        "\n",
        "ack_df\n",
        "\n",
        "\n",
        "ack_df.to_csv(\"acknowledgements.txt\", sep=\"\\t\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
