{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 121\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 2043 ACTION 0 NEXT REWARD 0 NEXT STATE 1915\n",
            "CURR STATE 1915 ACTION 1 NEXT REWARD 0 NEXT STATE 1788\n",
            "CURR STATE 1788 ACTION 0 NEXT REWARD 0 NEXT STATE 1660\n",
            "CURR STATE 1660 ACTION 0 NEXT REWARD 0 NEXT STATE 1532\n",
            "CURR STATE 1532 ACTION 1 NEXT REWARD 0 NEXT STATE 1405\n",
            "CURR STATE 1405 ACTION 0 NEXT REWARD 0 NEXT STATE 1269\n",
            "CURR STATE 1269 ACTION -1 NEXT REWARD 0 NEXT STATE 1132\n",
            "CURR STATE 1132 ACTION 0 NEXT REWARD 0 NEXT STATE 1004\n",
            "CURR STATE 1004 ACTION 0 NEXT REWARD 0 NEXT STATE 876\n",
            "CURR STATE 876 ACTION -1 NEXT REWARD 0 NEXT STATE 747\n",
            "CURR STATE 747 ACTION 1 NEXT REWARD 0 NEXT STATE 628\n",
            "CURR STATE 628 ACTION -1 NEXT REWARD 0 NEXT STATE 499\n",
            "CURR STATE 499 ACTION 1 NEXT REWARD 0 NEXT STATE 380\n",
            "CURR STATE 380 ACTION -1 NEXT REWARD 0 NEXT STATE 251\n",
            "CURR STATE 251 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION 1 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION 0 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION -1 NEXT REWARD 0 NEXT STATE 121\n",
            "CURR STATE 121 ACTION 0 NEXT REWARD 0 NEXT STATE 249\n",
            "CURR STATE 249 ACTION -1 NEXT REWARD 0 NEXT STATE 376\n",
            "CURR STATE 376 ACTION 1 NEXT REWARD 0 NEXT STATE 505\n",
            "CURR STATE 505 ACTION 1 NEXT REWARD 0 NEXT STATE 634\n",
            "CURR STATE 634 ACTION 1 NEXT REWARD 0 NEXT STATE 635\n",
            "CURR STATE 635 ACTION 0 NEXT REWARD 0 NEXT STATE 507\n",
            "CURR STATE 507 ACTION 1 NEXT REWARD 0 NEXT STATE 380\n",
            "CURR STATE 380 ACTION 0 NEXT REWARD 0 NEXT STATE 252\n",
            "CURR STATE 252 ACTION 1 NEXT REWARD 0 NEXT STATE 125\n",
            "CURR STATE 125 ACTION -1 NEXT REWARD 0 NEXT STATE 116\n",
            "CURR STATE 116 ACTION 0 NEXT REWARD 0 NEXT STATE 116\n",
            "CURR STATE 116 ACTION 1 NEXT REWARD 0 NEXT STATE 117\n",
            "CURR STATE 117 ACTION 0 NEXT REWARD 0 NEXT STATE 109\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import random\n",
        "import csv\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)  \n",
        "\n",
        "log_file = \"log-random.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\"])\n",
        "\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        writer.writerow([curr_state, curr_action, next_reward, next_state])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 6 ACTION -1 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 0 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION -1 NEXT REWARD 0 NEXT STATE 3\n",
            "CURR STATE 3 ACTION 1 NEXT REWARD 0 NEXT STATE 12\n",
            "CURR STATE 12 ACTION 1 NEXT REWARD 0 NEXT STATE 13\n",
            "CURR STATE 13 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION 0 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION 1 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION 0 NEXT REWARD 0 NEXT STATE 5\n",
            "CURR STATE 5 ACTION -1 NEXT REWARD 0 NEXT STATE 4\n",
            "CURR STATE 4 ACTION -1 NEXT REWARD 0 NEXT STATE 3\n",
            "CURR STATE 3 ACTION 1 NEXT REWARD 0 NEXT STATE 12\n",
            "CURR STATE 12 ACTION -1 NEXT REWARD 0 NEXT STATE 11\n",
            "CURR STATE 11 ACTION 1 NEXT REWARD 0 NEXT STATE 20\n",
            "CURR STATE 20 ACTION 0 NEXT REWARD 0 NEXT STATE 20\n",
            "CURR STATE 20 ACTION -1 NEXT REWARD 0 NEXT STATE 19\n",
            "CURR STATE 19 ACTION -1 NEXT REWARD 0 NEXT STATE 26\n",
            "CURR STATE 26 ACTION 0 NEXT REWARD 0 NEXT STATE 34\n",
            "CURR STATE 34 ACTION 1 NEXT REWARD 0 NEXT STATE 43\n",
            "CURR STATE 43 ACTION -1 NEXT REWARD 0 NEXT STATE 50\n",
            "CURR STATE 50 ACTION 1 NEXT REWARD 0 NEXT STATE 59\n",
            "CURR STATE 59 ACTION 0 NEXT REWARD 0 NEXT STATE 67\n",
            "CURR STATE 67 ACTION 0 NEXT REWARD 0 NEXT STATE 75\n",
            "CURR STATE 75 ACTION 1 NEXT REWARD 0 NEXT STATE 84\n",
            "CURR STATE 84 ACTION -1 NEXT REWARD 0 NEXT STATE 83\n",
            "CURR STATE 83 ACTION 1 NEXT REWARD 0 NEXT STATE 92\n",
            "CURR STATE 92 ACTION -1 NEXT REWARD 0 NEXT STATE 91\n",
            "CURR STATE 91 ACTION 0 NEXT REWARD 0 NEXT STATE 99\n",
            "CURR STATE 99 ACTION 1 NEXT REWARD 0 NEXT STATE 108\n",
            "CURR STATE 108 ACTION 0 NEXT REWARD 0 NEXT STATE 108\n",
            "CURR STATE 108 ACTION 1 NEXT REWARD 0 NEXT STATE 109\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import csv\n",
        "\n",
        "Q = {}  \n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "log_file = \"q-random.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "            \n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "        #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "\n",
        "        if next_reward > 0:\n",
        "            break\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25  \n",
        "\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "log_file = \"q-greedy.tsv\"\n",
        "\n",
        "with open(log_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "\n",
        "            if next_reward > 0:\n",
        "                break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards observed: 27\n"
          ]
        }
      ],
      "source": [
        "total_rewards = 0\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "        total_rewards += next_reward\n",
        "print(\"Total rewards observed:\", total_rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import csv\n",
        "\n",
        "policy_file = \"policy-greedy.tsv\"\n",
        "\n",
        "with open(policy_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\", \"action\"])\n",
        "\n",
        "    for state in simulator.initial_states + [simulator.terminal_state]:\n",
        "        actions = simulator.get_actions(state)\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        best_action = random.choice(best_actions)  \n",
        "        writer.writerow([state, best_action])\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import random\n",
        "import csv\n",
        "\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25  \n",
        "\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "num_episodes = 512  \n",
        "max_steps = 1024\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    initial_state = simulator.sample_initial_state()\n",
        "    state = initial_state\n",
        "    for step in range(max_steps):\n",
        "        actions = simulator.get_actions(state)\n",
        "        action = epsilon_greedy_policy(state, actions)\n",
        "        reward, next_state = simulator.get_next_reward_state(state, action)\n",
        "\n",
        "        old_value = Q.get((state, action), 0.0)\n",
        "        next_actions = simulator.get_actions(next_state)\n",
        "        max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "        new_value = old_value + alpha * (reward + gamma * max_next_q - old_value)\n",
        "        Q[(state, action)] = new_value\n",
        "\n",
        "        state = next_state\n",
        "        if reward > 0:\n",
        "            break  \n",
        "\n",
        "policy_file = \"policy-optimal.tsv\"\n",
        "\n",
        "with open(policy_file, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\", \"action\"])\n",
        "\n",
        "    all_states = set()\n",
        "    for (state, action) in Q.keys():\n",
        "        all_states.add(state)\n",
        "    all_states.add(simulator.terminal_state)\n",
        "\n",
        "    for state in all_states:\n",
        "        actions = simulator.get_actions(state)\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
        "        best_action = random.choice(best_actions)\n",
        "        writer.writerow([state, best_action])\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rollout_policy_from_q(state, policy, max_steps=1024):\n",
        "    total_reward = 0\n",
        "    for step in range(max_steps):\n",
        "        action = policy.get(state, 0)  \n",
        "        reward, next_state = simulator.get_next_reward_state(state, action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if reward > 0:  \n",
        "            break\n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = {}\n",
        "for (state, action) in Q.keys():\n",
        "    q_values = [Q.get((state, a), 0.0) for a in simulator.get_actions(state)]\n",
        "    max_q = max(q_values)\n",
        "    best_actions = [a for a, q in zip(simulator.get_actions(state), q_values) if q == max_q]\n",
        "    policy[state] = random.choice(best_actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward per initial state: 1.0\n"
          ]
        }
      ],
      "source": [
        "total_reward = 0\n",
        "for init_state in simulator.initial_states:\n",
        "    total_reward += rollout_policy_from_q(init_state, policy)\n",
        "\n",
        "average_reward = total_reward / len(simulator.initial_states)\n",
        "print(\"Average reward per initial state:\", average_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of positive rewards found: 17\n",
            "Terminal state reached at least once — learning signal present.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "filename = \"q-greedy.tsv\"  # or \"q-random.tsv\"\n",
        "reward_count = 0\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
        "    for row in reader:\n",
        "        if float(row[\"next_reward\"]) == 1:\n",
        "            reward_count += 1\n",
        "\n",
        "print(f\"Number of positive rewards found: {reward_count}\")\n",
        "if reward_count > 0:\n",
        "    print(\"Terminal state reached at least once — learning signal present.\")\n",
        "else:\n",
        "    print(\"No rewards found — the agent never reached the terminal state!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0: reached terminal at state 1088\n",
            "Episode 1: reached terminal at state 1088\n",
            "Episode 2: reached terminal at state 1088\n",
            "Episode 3: reached terminal at state 1088\n",
            "Episode 4: reached terminal at state 1088\n",
            "Episode 5: reached terminal at state 1088\n",
            "Episode 6: reached terminal at state 1088\n",
            "Episode 7: reached terminal at state 1088\n",
            "Episode 8: reached terminal at state 1088\n",
            "Episode 9: reached terminal at state 1088\n",
            "Episode 10: reached terminal at state 1088\n",
            "Episode 11: reached terminal at state 1088\n",
            "Episode 12: reached terminal at state 1088\n",
            "Episode 13: reached terminal at state 1088\n",
            "Episode 14: reached terminal at state 1088\n",
            "Episode 15: reached terminal at state 1088\n",
            "Episode 16: reached terminal at state 1088\n",
            "Episode 17: reached terminal at state 1088\n",
            "Episode 18: reached terminal at state 1088\n",
            "Episode 19: reached terminal at state 1088\n",
            "Episode 20: reached terminal at state 1088\n",
            "Episode 21: reached terminal at state 1088\n",
            "Episode 22: reached terminal at state 1088\n",
            "Episode 23: reached terminal at state 1088\n",
            "Episode 24: reached terminal at state 1088\n",
            "Episode 25: reached terminal at state 1088\n",
            "Episode 26: reached terminal at state 1088\n",
            "Episode 27: reached terminal at state 1088\n",
            "Episode 28: reached terminal at state 1088\n",
            "Episode 29: reached terminal at state 1088\n",
            "Episode 30: reached terminal at state 1088\n",
            "Episode 31: reached terminal at state 1088\n"
          ]
        }
      ],
      "source": [
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "        if next_reward == 1:\n",
        "            print(f\"Episode {episode}: reached terminal at state {next_state}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting numpy>=1.26.0 (from pandas)\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, numpy, pandas\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.4 pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "acknowledgements = [\n",
        "['1. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html', \n",
        " 'To learn more about Q learning fundamentals'],\n",
        "['2. https://davidstarsilver.wordpress.com/teaching/',\n",
        " 'Week 2 slides/code explain deterministic Q-learning, α=1, γ<1 updates'],\n",
        "['3. http://incompleteideas.net/book/RLbook2020.pdf', 'Chapter 6- Q learning'],\n",
        "['4. https://gymnasium.farama.org/tutorials/training_agents/frozenlake_q_learning/#sphx-glr-tutorials-training-agents-frozenlake-q-learning-py', 'Learning more about Q Learning Code']\n",
        "]\n",
        "\n",
        "\n",
        "columns = ['Source', 'Used For/Reason']\n",
        "\n",
        "ack_df = pd.DataFrame(acknowledgements, columns=columns)\n",
        "\n",
        "ack_df\n",
        "\n",
        "\n",
        "ack_df.to_csv(\"acknowledgements.txt\", sep=\"\\t\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
